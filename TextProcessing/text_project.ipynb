{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트 정보 처리 프로젝트\n",
        "###### 참고 사이트\n",
        "ㆍ https://wikidocs.net/22530  \n",
        "ㆍ https://mr-doosun.tistory.com/24  \n",
        "ㆍ https://nicola-ml.tistory.com/63  \n",
        "ㆍ https://ahnsun98.tistory.com/35  (stopword)  "
      ],
      "metadata": {
        "id": "EfW_R2cl86PE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.9.0 --user    #실행 후 런타임 재시작 필요\n",
        "!pip install rhinoMorph\n",
        "import rhinoMorph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "86qJeAeW1zHH",
        "outputId": "0253046c-de51-49c9-b27b-ad0bdd835f2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.9.0\n",
            "  Downloading torchtext-0.9.0-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 16.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (2.23.0)\n",
            "Collecting torch==1.8.0\n",
            "  Downloading torch-1.8.0-cp38-cp38-manylinux1_x86_64.whl (735.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.9.0) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0->torchtext==0.9.0) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.9.0) (2.10)\n",
            "Installing collected packages: torch, torchtext\n",
            "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.8.0 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.8.0 torchtext-0.9.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rhinoMorph\n",
            "  Downloading rhinoMorph-4.0.1.6-py3-none-any.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 25.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: rhinoMorph\n",
            "Successfully installed rhinoMorph-4.0.1.6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d296199ae58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install torchtext==0.9.0 --user    #실행 후 런타임 재시작 필요'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install rhinoMorph'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrhinoMorph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/rhinoMorph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjpype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstartRhino\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jpype'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from torchtext.legacy import data\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install konlpy\n",
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "import re\n",
        "import json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import os\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# stopword 처리\n",
        "from konlpy.tag import Okt as okt\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aACuc3YdPs-N",
        "outputId": "ddb9b15d-2985-4bce-c200-6d4d6c31babc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.8/dist-packages (from konlpy) (1.21.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.4.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[K     |████████████████████████████████| 465 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from konlpy) (4.9.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.4.1 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4IIucYeQs21",
        "outputId": "552f721a-d383-4769-a8a3-65925b501caf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc908baee30>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "KERNEL_SIZE = 5\n",
        "\n",
        "def tokenizer(text):\n",
        "  token = kkma.morphs(text)\n",
        "  if len(token) < KERNEL_SIZE:\n",
        "    for i in range(0, KERNEL_SIZE - len(token)):\n",
        "      token.append(\"<PAD>\") #커널 사이즈보다 문장의 길이가 작은 경우 에러 방지\n",
        "  return token"
      ],
      "metadata": {
        "id": "55F1-U_2QS1T"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1이 긍정, 0이 부정"
      ],
      "metadata": {
        "id": "fJcWQUTvrajb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/Train.txt', 'r')\n",
        "\n",
        "data = f.readlines()\n",
        "\n",
        "print(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt5h9jeURxO0",
        "outputId": "d1371dd8-ced6-4e40-9a37-472c39271637"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\t배공빠르고 굿\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 셋\n",
        "train_set = data[0 : int(len(data) * 0.7)]\n",
        "test_set = data[int(len(data) * 0.7) :]\n",
        "\n",
        "print(len(train_set))\n",
        "print(len(test_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og0yKQffQVqo",
        "outputId": "f7f871c3-6554-4529-e2d7-6bed908dd045"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140000\n",
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set[0][0])\n",
        "print(train_set[0][2:])\n",
        "print(test_set[0][0])\n",
        "print(test_set[0][2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYK9a6pirKl_",
        "outputId": "ef71d35c-4edc-4ecd-b582-a75fd75db690"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "배공빠르고 굿\n",
            "\n",
            "4\n",
            "빠른 배송만족하고 패드 색감이나 감촉 다 좋아요!!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function"
      ],
      "metadata": {
        "id": "Sk9DY1X7velf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_set))\n",
        "print(len(test_set))\n",
        "\n",
        "print(train_set[0][0])\n",
        "print(train_set[0][2:])\n",
        "print(test_set[0][0])\n",
        "print(test_set[0][2:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAYmx6lGZNVa",
        "outputId": "3c91dcf6-2903-41bb-d7e4-fa14e8e91107"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "140000\n",
            "60000\n",
            "5\n",
            "배공빠르고 굿\n",
            "\n",
            "4\n",
            "빠른 배송만족하고 패드 색감이나 감촉 다 좋아요!!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 긍정 : 1, 부정 : 0\n",
        "train_text = []\n",
        "train_senti = []\n",
        "test_text = []\n",
        "test_senti = []\n",
        "\n",
        "for i in range(0, len(train_set)):\n",
        "  temp = train_set[i][2:]\n",
        "  train_text.append(temp[0:len(temp)-1])\n",
        "\n",
        "  if((train_set[i][0] == '1') or (train_set[i][0] == '2')):\n",
        "    train_senti.append(0)\n",
        "  else:\n",
        "    train_senti.append(1)\n",
        "\n",
        "\n",
        "for i in range(0, len(test_set)):\n",
        "  temp = test_set[i][2:]\n",
        "  test_text.append(temp[0:len(temp)-1])\n",
        "  \n",
        "  if((test_set[i][0] == '1') or (test_set[i][0] == '2')):\n",
        "    test_senti.append(0)\n",
        "  else:\n",
        "    test_senti.append(1)\n",
        "\n",
        "\n",
        "print(train_text[0:20])\n",
        "print(train_senti[0:20])\n",
        "print(test_text[0:20])\n",
        "print(test_senti[0:20])\n",
        "\n",
        "print(type(train_text[0]))\n",
        "print(type(train_senti[0]))\n",
        "\n",
        "#train_text를 파일로 쓰기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_data.txt\", \"w\")\n",
        "\n",
        "for i in range(0, len(train_text)):\n",
        "  data = train_text[i] + \"\\n\"\n",
        "  f.write(data)\n",
        "\n",
        "f.close()\n",
        "\n",
        "#train_senti를 파일로 쓰기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_senti.txt\", \"w\")\n",
        "\n",
        "for i in range(0, len(train_senti)):\n",
        "  data = str(train_senti[i]) + \"\\n\"\n",
        "  f.write(data)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiiWkU0IZ5n0",
        "outputId": "53dd77eb-6ad0-4e23-a03a-78c4cd81de90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['배공빠르고 굿', '택배가 엉망이네용 저희집 밑에층에 말도없이 놔두고가고', '아주좋아요 바지 정말 좋아서2개 더 구매했어요 이가격에 대박입니다. 바느질이 조금 엉성하긴 하지만 편하고 가성비 최고예요.', '선물용으로 빨리 받아서 전달했어야 하는 상품이었는데 머그컵만 와서 당황했습니다. 전화했더니 바로주신다했지만 배송도 누락되어있었네요.. 확인안하고 바로 선물했으면 큰일날뻔했네요..이렇게 배송이 오래걸렸으면 사는거 다시 생각했을거같아요 아쉽네요..', '민트색상 예뻐요. 옆 손잡이는 거는 용도로도 사용되네요 ㅎㅎ', '비추합니다 계란 뒤집을 때 완전 불편해요 ㅠㅠ 코팅도 묻어나고 보기엔 예쁘고 실용적으로 보였는데 생각보다 진짜 별로입니다.', '주문을 11월6에 시켰는데 11월16일에 배송이 왔네요 ㅎㅎㅎ 여기 회사측과는 전화도 안되고 아무런 연락을 받을수가 없으니 답답하신 분들은 다른곳에서 사시는거 추천드립니다', '넉넉한 길이로 주문했는데도 안 맞네요 별로예요', '보폴이 계속 때처럼 나오다가 지금은 안나네요~', '110인데 전문속옷브랜드 위생팬티105보다 작은듯해요. 불편해요. 밴딩부분이 다 신축성없는 일반실로 되어있어 빅사이즈임에도 빅사이즈같지않아요. 입고벗을때 편하게 밴딩부분이 늘어나고 입었을때도 밴딩이 잡아주어야하는데 말이죠.', '사이즈도 딱이고 귀엽고 넘 좋아요 ㅎㅎ', '베이지 색 구매했는데 약간 살색에 가까워요', '화면빨인가봐요;; 노란컬러가 돋보여요;; 저렴한맛에 그냥 씁니다', '별루 ㅏㅛㅇ치ㅗ티ㅓ치ㅗ탛캏타ㅗ티ㅗ티ㅗ티ㅛ티ㅛ티ㅗㅗㅗ치ㅕ치ㅕ쳐ㅣ처ㅣ치ㅓ처ㅣ펴ㅣ쳐ㅣ치ㅕㅐㅛㅌ쵸ㅔ려ㅔㅎ', '촉감도 좋고 무게감이나 핏도 편합니다', '불멍하기좋고 사이즈도 너무 좋아요', '재구매 친구들이 좋은 향 난다고 해요', '실내에서 신는건지 몰랐어요', '재구매 다 좋은데 하나가 이상하네요', '가게를 운영하는 사장님께서 대신 구매 해달라고 하셔서 구매하게되었는데 가게에 달아놓으니까 이쁘네요']\n",
            "[1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1]\n",
            "['빠른 배송만족하고 패드 색감이나 감촉 다 좋아요!!', '지점마다 맛의차이가 있어서 걱정했는데 고속터미널 2층 맛있으요', '영상보고 해도 기포 생기네요 먼지 한톨 없이 하려고 샤워 후 수증기때문에 먼지 없는 상태에서했는데 먼지가 문제가 아니라 그냥 기포가 생기네요 비추천합니다', '사용해보니 청결하니 만족합니다.', '간편하게 채수 만들수있어서 좋아요~ 이유식 만드는거도 버거운 저에게는 꼭 필요한 제품이네요 ㅎㅎ', '최악입니다최악입니다 내구성최악에 돈만날렸어요', '너무붙어서 입기불변합니다', '방수매트라 관리가 편하다는 점에서 별 다섯개입니다 :) 식사후 흘린 음식물을 치워도 얼룩이나 이염이 되지 않으니 편하고 좋네요 흔하지 않은 디자인과 컬러라 포인트주기 좋아요 식기를 올렸을때 더 예쁜 매트라 만족합니다 :)', '애기가 너무 좋아해요 감사합니다', '3pieces와 디자인은 괜찮으나 프로모션 미공지, 거듭된 문의에 형식적 답변, 배송 보름에 상태 알 수 없음. 가벼워서 밀림, 냄새가 있으며 내장재의 안정성에 의심이 갑니다.', '싼게 비지떡 이네요 서랍 내부 필름지도 없이 MDF그냥 제단해서 사용하고 기본적인 일반 레일도 없이 장난감도 아니고 너무 하네요 저렴한 가격이지만 최소한도 안도는 나무 상자입니다', '계속 잘 쓰고 있습니다~', '사무실 간식으로 아주 좋아요 소포장이라 편리해요', '그저그저 그런거 같아요. 말하기도 귀찮응', '부모님이 전업투자 선언해서 주문했는데 만족합니다 가격 성능 좋네요', '넥타이 두꺼우면 끼우기 힘드네요 ㅠㅠ', '너무 비싸다는 점..0', '한번입어보고 그대로 버렸어요 사이즈도 불편하고 너무 까칠해서 못입어요', '대창은 아주맛있었어요 그런데 곱창에선 돌이나왔어요', '여행용으로샀어요 만족']\n",
            "[1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]\n",
            "<class 'str'>\n",
            "<class 'int'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 피처 벡터화\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(min_df=5).fit(train_text)\n",
        "X_train = vect.transform(train_text)\n",
        "\n",
        "feature_names = vect.get_feature_names()\n",
        "print(\"특성 개수:\", len(feature_names))\n",
        "print(\"처음 20개 특성:\\m\", feature_names[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88M0T7iHdorD",
        "outputId": "57f21c1b-80b8-4486-b111-0b03f20fe2f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특성 개수: 23526\n",
            "처음 20개 특성:\\m ['000', '000원', '000원에', '010', '07', '0점', '10', '100', '1000원', '1000원짜리', '100g', '100개', '100사이즈', '100일', '100장', '100점', '100프로', '105', '105사이즈', '10mm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = pd.Series(train_senti)\n",
        "scores = cross_val_score(LogisticRegression(solver=\"liblinear\"), X_train, y_train, cv=5)\n",
        "print('교차 검증 점수:', scores)\n",
        "print('교차 검증 점수 평균:', scores.mean())\n",
        "print('\\n')\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 3, 5]}\n",
        "grid = GridSearchCV(LogisticRegression(solver=\"liblinear\"), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최고 교차 검증 점수:\", round(grid.best_score_, 3))\n",
        "print(\"최적의 매개변수:\", grid.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt2n4kjee97W",
        "outputId": "9f9ef698-8111-447e-a34c-a441d659838b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "교차 검증 점수: [0.87453571 0.87728571 0.87510714 0.87517857 0.87492857]\n",
            "교차 검증 점수 평균: 0.8754071428571428\n",
            "\n",
            "\n",
            "최고 교차 검증 점수: 0.875\n",
            "최적의 매개변수: {'C': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test 데이터로 검증\n",
        "X_test = vect.transform(test_text)\n",
        "y_test = pd.Series(test_senti)\n",
        "print(\"테스트 데이터 점수:\", grid.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjiUVzwagyU",
        "outputId": "ff237331-d1da-4a4d-dd79-8e8a3ccdd0ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 데이터 점수: 0.8764833333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 런타임 재시작 필요\n",
        "# ver.3 (새로운 데이터를 파일에서 읽어 리스트로 생성하여 전달)\n",
        "\n",
        "# train_data는 gdrive에서 train_save_data 읽기 & stopword(불용어) 처리\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_data.txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# stopword 읽기 #https://ahnsun98.tistory.com/35\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/korean_stopword.txt\", \"r\")\n",
        "stopwords = f.readlines()\n",
        "f.close()\n",
        "\n",
        "\n",
        "word_tokens = []\n",
        "stop_tokens = []\n",
        "\n",
        "for stopword in stopwords:\n",
        "  stop_tokens.append(stopword[:len(stopword) - 1])\n",
        "for line in lines:\n",
        "  if line not in stop_tokens:\n",
        "    word_tokens.append(line[:len(line) - 1])\n",
        "\n",
        "\n",
        "\n",
        "# print(lines[0:5])\n",
        "# print(word_tokens[0:5])   # 위의 train_text와 동일\n",
        "# print(type(word_tokens[0])) # str\n",
        "# print(stopwords[0:10])\n",
        "# print(stop_tokens[0:10])\n",
        "\n",
        "# train_data의 sentimental 데이터 읽기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_senti.txt\", \"r\")\n",
        "sentis = f.readlines()\n",
        "f.close()\n",
        "\n",
        "sentiment = []\n",
        "for senti in sentis:\n",
        "  sentiment.append(int(senti[:len(senti) - 1]))\n",
        "\n",
        "# print(sentiment)    #위의 train_senti와 동일\n",
        "# print(type(sentiment[0]))   # int\n",
        "\n",
        "# 새로운 데이터로 검증\n",
        "import rhinoMorph\n",
        "rn = rhinoMorph.startRhino()\n",
        "#print('rn\\n',rn)\n",
        "new_input = ''\n",
        "\n",
        "# 새로운 데이터를 파일에서 읽기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/final_test_data.txt\", \"r\")\n",
        "new_inputs = f.readlines()\n",
        "f.close()\n",
        "\n",
        "new_input_list = []\n",
        "for new_input in new_inputs:\n",
        "  new_input_list.append(new_input)\n",
        "\n",
        "#print(new_input_list[0])\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "\n",
        "\n",
        "# 위 반복\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "X_train = vect.transform(word_tokens)\n",
        "y_train = pd.Series(sentiment)\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 3, 5]}\n",
        "grid = GridSearchCV(LogisticRegression(solver=\"liblinear\"), param_grid, cv=5)\n",
        "# grid.fit(word_tokens, sentiment)  \n",
        "grid.fit(X_train, y_train)  \n",
        "\n",
        "# 위 반복\n",
        "\n",
        "senti_result = []\n",
        "\n",
        "# @@@ 데이터 여러 개\n",
        "for new_input in new_input_list:\n",
        "  # 입력 데이터 형태소 분석하기\n",
        "  inputdata = []\n",
        "  morphed_input = rhinoMorph.onlyMorph_list(rn, new_input, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
        "  morphed_input = ' '.join(morphed_input)                     # 한 개의 문자열로 만들기\n",
        "  inputdata.append(morphed_input)                               # 분석 결과를 리스트로 만들기\n",
        "  X_input = vect.transform(inputdata)\n",
        "\n",
        "  #print(float(grid.predict(X_input)))\n",
        "  result = grid.predict(X_input) # 0은 부정,1은 긍정\n",
        "  #print(result)\n",
        "  #print(type(result))\n",
        "  if result == 0.0:\n",
        "      #print(\"부정적인 글입니다\")\n",
        "      senti_result.append(0)\n",
        "  else:\n",
        "      #print(\"긍정적인 글입니다\")\n",
        "      senti_result.append(1)\n",
        "\n",
        "# @@@ 데이터 여러 개\n",
        "\n",
        "\n",
        "#print(senti_result[0 : 10])\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/ProjectResult.txt\", \"w\")\n",
        "for i in range(0, len(senti_result)):\n",
        "  data = str(senti_result[i]) + \"\\n\"\n",
        "  f.write(data)\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mShUhQdUATln",
        "outputId": "33e4fdf3-260f-4efd-ecec-5dc51fff8900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "filepath:  /usr/local/lib/python3.8/dist-packages\n",
            "classpath:  /usr/local/lib/python3.8/dist-packages/rhinoMorph/lib/rhino.jar\n",
            "JVM is already started~\n",
            "RHINO started!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 런타임 재시작 필요\n",
        "# ver.1\n",
        "\n",
        "# train_data는 gdrive에서 train_save_data 읽기 & stopword(불용어) 처리\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_data.txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# stopword 읽기 #https://ahnsun98.tistory.com/35\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/korean_stopword.txt\", \"r\")\n",
        "stopwords = f.readlines()\n",
        "f.close()\n",
        "\n",
        "\n",
        "word_tokens = []\n",
        "stop_tokens = []\n",
        "\n",
        "for stopword in stopwords:\n",
        "  stop_tokens.append(stopword[:len(stopword) - 1])\n",
        "for line in lines:\n",
        "  if line not in stop_tokens:\n",
        "    word_tokens.append(line[:len(line) - 1])\n",
        "\n",
        "\n",
        "\n",
        "# print(lines[0:5])\n",
        "print(word_tokens[0:5])   # 위의 train_text와 동일\n",
        "print(type(word_tokens[0])) # str\n",
        "# print(stopwords[0:10])\n",
        "print(stop_tokens[0:10])\n",
        "\n",
        "# train_data의 sentimental 데이터 읽기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_senti.txt\", \"r\")\n",
        "sentis = f.readlines()\n",
        "f.close()\n",
        "\n",
        "sentiment = []\n",
        "for senti in sentis:\n",
        "  sentiment.append(int(senti[:len(senti) - 1]))\n",
        "\n",
        "print(sentiment)    #위의 train_senti와 동일\n",
        "print(type(sentiment[0]))   # int\n",
        "\n",
        "# 새로운 데이터로 검증\n",
        "import rhinoMorph\n",
        "rn = rhinoMorph.startRhino()\n",
        "print('rn\\n',rn)\n",
        "new_input = '재구매 편리하게 사용할 수 있어요'\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "\n",
        "# 입력 데이터 형태소 분석하기\n",
        "inputdata = []\n",
        "morphed_input = rhinoMorph.onlyMorph_list(rn, new_input, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
        "morphed_input = ' '.join(morphed_input)                     # 한 개의 문자열로 만들기\n",
        "inputdata.append(morphed_input)                               # 분석 결과를 리스트로 만들기\n",
        "X_input = vect.transform(inputdata)\n",
        "\n",
        "# 위 반복\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "X_train = vect.transform(word_tokens)\n",
        "y_train = pd.Series(sentiment)\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 3, 5]}\n",
        "grid = GridSearchCV(LogisticRegression(solver=\"liblinear\"), param_grid, cv=5)\n",
        "# grid.fit(word_tokens, sentiment)  \n",
        "grid.fit(X_train, y_train)  \n",
        "\n",
        "# 위 반복\n",
        "\n",
        "print(float(grid.predict(X_input)))\n",
        "result = grid.predict(X_input) # 0은 부정,1은 긍정\n",
        "print(result)\n",
        "print(type(result))\n",
        "if result == 0.0:\n",
        "    print(\"부정적인 글입니다\")\n",
        "else:\n",
        "    print(\"긍정적인 글입니다\")"
      ],
      "metadata": {
        "id": "W8nWhisDarZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 런타임 재시작 필요\n",
        "# ver.2 (새로운 데이터를 리스트로 받음)\n",
        "\n",
        "# train_data는 gdrive에서 train_save_data 읽기 & stopword(불용어) 처리\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_data.txt\", \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "\n",
        "# stopword 읽기 #https://ahnsun98.tistory.com/35\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/korean_stopword.txt\", \"r\")\n",
        "stopwords = f.readlines()\n",
        "f.close()\n",
        "\n",
        "\n",
        "word_tokens = []\n",
        "stop_tokens = []\n",
        "\n",
        "for stopword in stopwords:\n",
        "  stop_tokens.append(stopword[:len(stopword) - 1])\n",
        "for line in lines:\n",
        "  if line not in stop_tokens:\n",
        "    word_tokens.append(line[:len(line) - 1])\n",
        "\n",
        "\n",
        "\n",
        "# print(lines[0:5])\n",
        "# print(word_tokens[0:5])   # 위의 train_text와 동일\n",
        "# print(type(word_tokens[0])) # str\n",
        "# print(stopwords[0:10])\n",
        "# print(stop_tokens[0:10])\n",
        "\n",
        "# train_data의 sentimental 데이터 읽기\n",
        "f = open(\"/content/drive/MyDrive/Colab Notebooks/텍스트정보처리/content/train_save_senti.txt\", \"r\")\n",
        "sentis = f.readlines()\n",
        "f.close()\n",
        "\n",
        "sentiment = []\n",
        "for senti in sentis:\n",
        "  sentiment.append(int(senti[:len(senti) - 1]))\n",
        "\n",
        "# print(sentiment)    #위의 train_senti와 동일\n",
        "# print(type(sentiment[0]))   # int\n",
        "\n",
        "# 새로운 데이터로 검증\n",
        "import rhinoMorph\n",
        "rn = rhinoMorph.startRhino()\n",
        "print('rn\\n',rn)\n",
        "new_input = ''\n",
        "new_input_list = ['배송 한달걸림 나빠요', '너무 좋아요 튼튼하고 바퀴도 있어서 가지고 다니기도 편해요 너무 잘 삿어요', '역시 맛있습니다 주기적으로 계속 사먹어요!']\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "\n",
        "\n",
        "# 위 반복\n",
        "vect = CountVectorizer(min_df=5).fit(word_tokens)\n",
        "X_train = vect.transform(word_tokens)\n",
        "y_train = pd.Series(sentiment)\n",
        "\n",
        "param_grid = {'C': [0.01, 0.1, 1, 3, 5]}\n",
        "grid = GridSearchCV(LogisticRegression(solver=\"liblinear\"), param_grid, cv=5)\n",
        "# grid.fit(word_tokens, sentiment)  \n",
        "grid.fit(X_train, y_train)  \n",
        "\n",
        "# 위 반복\n",
        "\n",
        "\n",
        "\n",
        "# @@@ 데이터 여러 개\n",
        "for new_input in new_input_list:\n",
        "  # 입력 데이터 형태소 분석하기\n",
        "  inputdata = []\n",
        "  morphed_input = rhinoMorph.onlyMorph_list(rn, new_input, pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
        "  morphed_input = ' '.join(morphed_input)                     # 한 개의 문자열로 만들기\n",
        "  inputdata.append(morphed_input)                               # 분석 결과를 리스트로 만들기\n",
        "  X_input = vect.transform(inputdata)\n",
        "\n",
        "  print(float(grid.predict(X_input)))\n",
        "  result = grid.predict(X_input) # 0은 부정,1은 긍정\n",
        "  print(result)\n",
        "  print(type(result))\n",
        "  if result == 0.0:\n",
        "      print(\"부정적인 글입니다\")\n",
        "  else:\n",
        "      print(\"긍정적인 글입니다\")\n",
        "\n",
        "# @@@ 데이터 여러 개\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U4FT-jFh8uTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}